{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de9b66b-3265-4809-be3e-14c28bcb7664",
   "metadata": {},
   "source": [
    "**Part A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d5422e-1466-462d-af7c-53d28eb081cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/50\n",
      "721/721 [==============================] - 8s 11ms/step - loss: 10295.8978\n",
      "Epoch 2/50\n",
      "721/721 [==============================] - 0s 171us/step - loss: 2866.0525\n",
      "Epoch 3/50\n",
      "721/721 [==============================] - 0s 183us/step - loss: 2185.9604\n",
      "Epoch 4/50\n",
      "721/721 [==============================] - 0s 206us/step - loss: 1835.7250\n",
      "Epoch 5/50\n",
      "721/721 [==============================] - 0s 235us/step - loss: 1538.0572\n",
      "Epoch 6/50\n",
      "721/721 [==============================] - 0s 220us/step - loss: 1297.5963\n",
      "Epoch 7/50\n",
      "721/721 [==============================] - 0s 187us/step - loss: 1069.7868\n",
      "Epoch 8/50\n",
      "721/721 [==============================] - 0s 145us/step - loss: 904.0746\n",
      "Epoch 9/50\n",
      "721/721 [==============================] - 0s 143us/step - loss: 767.2274\n",
      "Epoch 10/50\n",
      "721/721 [==============================] - 0s 139us/step - loss: 661.3165\n",
      "Epoch 11/50\n",
      "721/721 [==============================] - 0s 137us/step - loss: 578.4598\n",
      "Epoch 12/50\n",
      "721/721 [==============================] - 0s 137us/step - loss: 512.6434\n",
      "Epoch 13/50\n",
      "721/721 [==============================] - 0s 143us/step - loss: 460.2598\n",
      "Epoch 14/50\n",
      "721/721 [==============================] - 0s 133us/step - loss: 415.8492\n",
      "Epoch 15/50\n",
      "721/721 [==============================] - 0s 134us/step - loss: 380.1232\n",
      "Epoch 16/50\n",
      "721/721 [==============================] - 0s 146us/step - loss: 351.1043\n",
      "Epoch 17/50\n",
      "721/721 [==============================] - 0s 127us/step - loss: 329.6972\n",
      "Epoch 18/50\n",
      "721/721 [==============================] - 0s 154us/step - loss: 310.1211\n",
      "Epoch 19/50\n",
      "721/721 [==============================] - 0s 139us/step - loss: 296.6266\n",
      "Epoch 20/50\n",
      "721/721 [==============================] - 0s 130us/step - loss: 282.0128\n",
      "Epoch 21/50\n",
      "721/721 [==============================] - 0s 117us/step - loss: 271.6712\n",
      "Epoch 22/50\n",
      "721/721 [==============================] - 0s 136us/step - loss: 263.6926\n",
      "Epoch 23/50\n",
      "721/721 [==============================] - 0s 136us/step - loss: 257.3606\n",
      "Epoch 24/50\n",
      "721/721 [==============================] - 0s 142us/step - loss: 245.3138\n",
      "Epoch 25/50\n",
      "721/721 [==============================] - 0s 132us/step - loss: 238.0986\n",
      "Epoch 26/50\n",
      "721/721 [==============================] - 0s 143us/step - loss: 230.3311\n",
      "Epoch 27/50\n",
      "721/721 [==============================] - 0s 142us/step - loss: 223.3655\n",
      "Epoch 28/50\n",
      "721/721 [==============================] - 0s 128us/step - loss: 217.9686\n",
      "Epoch 29/50\n",
      "721/721 [==============================] - 0s 120us/step - loss: 217.0234\n",
      "Epoch 30/50\n",
      "721/721 [==============================] - 0s 136us/step - loss: 209.4206\n",
      "Epoch 31/50\n",
      "721/721 [==============================] - 0s 131us/step - loss: 200.5636\n",
      "Epoch 32/50\n",
      "721/721 [==============================] - 0s 139us/step - loss: 197.0954\n",
      "Epoch 33/50\n",
      "721/721 [==============================] - 0s 132us/step - loss: 194.2272\n",
      "Epoch 34/50\n",
      "721/721 [==============================] - 0s 134us/step - loss: 188.3922\n",
      "Epoch 35/50\n",
      "721/721 [==============================] - 0s 136us/step - loss: 185.0235\n",
      "Epoch 36/50\n",
      "721/721 [==============================] - 0s 248us/step - loss: 181.9660\n",
      "Epoch 37/50\n",
      "721/721 [==============================] - 0s 173us/step - loss: 184.6412\n",
      "Epoch 38/50\n",
      "721/721 [==============================] - 0s 170us/step - loss: 177.5390\n",
      "Epoch 39/50\n",
      "721/721 [==============================] - 0s 129us/step - loss: 174.9671\n",
      "Epoch 40/50\n",
      "721/721 [==============================] - 0s 116us/step - loss: 165.9596\n",
      "Epoch 41/50\n",
      "721/721 [==============================] - 0s 115us/step - loss: 165.4870\n",
      "Epoch 42/50\n",
      "721/721 [==============================] - 0s 142us/step - loss: 162.2344\n",
      "Epoch 43/50\n",
      "721/721 [==============================] - 0s 123us/step - loss: 158.0941\n",
      "Epoch 44/50\n",
      "721/721 [==============================] - 0s 127us/step - loss: 155.4412\n",
      "Epoch 45/50\n",
      "721/721 [==============================] - 0s 124us/step - loss: 151.9196\n",
      "Epoch 46/50\n",
      "721/721 [==============================] - 0s 124us/step - loss: 153.1275\n",
      "Epoch 47/50\n",
      "721/721 [==============================] - 0s 119us/step - loss: 147.4400\n",
      "Epoch 48/50\n",
      "721/721 [==============================] - 0s 125us/step - loss: 144.6609\n",
      "Epoch 49/50\n",
      "721/721 [==============================] - 0s 142us/step - loss: 142.2246\n",
      "Epoch 50/50\n",
      "721/721 [==============================] - 0s 144us/step - loss: 139.9150\n",
      "309/309 [==============================] - 3s 11ms/step\n",
      "143.694154610907 0.0\n",
      "MSE 1: 98.76165450506612\n",
      "MSE 2: 97.34117694348579\n",
      "MSE 3: 65.39932290481518\n",
      "MSE 4: 88.51992104045782\n",
      "MSE 5: 73.65611417702486\n",
      "MSE 6: 74.4673141923923\n",
      "MSE 7: 105.76036086591702\n",
      "MSE 8: 62.19351782690746\n",
      "MSE 9: 66.99998691398349\n",
      "MSE 10: 59.470070749424806\n",
      "MSE 11: 58.212781245654455\n",
      "MSE 12: 53.91310501098633\n",
      "MSE 13: 59.91218016294214\n",
      "MSE 14: 54.51611031837834\n",
      "MSE 15: 56.44574748659597\n",
      "MSE 16: 45.935126603999954\n",
      "MSE 17: 49.787170990385285\n",
      "MSE 18: 51.2083862946643\n",
      "MSE 19: 46.42250342507964\n",
      "MSE 20: 50.0295612248788\n",
      "MSE 21: 52.85866472559068\n",
      "MSE 22: 45.957126555705145\n",
      "MSE 23: 46.255930212323335\n",
      "MSE 24: 51.170276530738015\n",
      "MSE 25: 50.00379690769035\n",
      "MSE 26: 46.51035331059428\n",
      "MSE 27: 47.17628639492788\n",
      "MSE 28: 47.40351765209803\n",
      "MSE 29: 65.07863360161149\n",
      "MSE 30: 52.51939745165384\n",
      "MSE 31: 50.02889444295642\n",
      "MSE 32: 46.14589873255264\n",
      "MSE 33: 50.35717138889152\n",
      "MSE 34: 45.43493674565288\n",
      "MSE 35: 45.30633164686678\n",
      "MSE 36: 53.55049171571207\n",
      "MSE 37: 48.805486987709614\n",
      "MSE 38: 49.90294044766225\n",
      "MSE 39: 46.58332442002775\n",
      "MSE 40: 46.18015525101844\n",
      "MSE 41: 60.97020074541901\n",
      "MSE 42: 44.5398072177924\n",
      "MSE 43: 44.77234657065382\n",
      "MSE 44: 63.7389170328776\n",
      "MSE 45: 59.40262265498584\n",
      "MSE 46: 64.33977957913791\n",
      "MSE 47: 52.23090692168301\n",
      "MSE 48: 48.09479720229856\n",
      "MSE 49: 55.61642575804084\n",
      "MSE 50: 50.76951868248603\n",
      "\n",
      "\n",
      "Below is the mean and standard deviation of 50 mean squared errors without normalized data. Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 57.01314172201207\n",
      "Standard Deviation: 14.176692077047205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Import Pandas library for data manipulation\n",
    "import numpy as np   # Import NumPy library for numerical operations\n",
    "import keras         # Import Keras library for building neural networks\n",
    "\n",
    "from keras.models import Sequential  # Import Sequential model from Keras for building models sequentially\n",
    "from keras.layers import Dense       # Import Dense layer from Keras for fully connected layers\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split function from scikit-learn for splitting data\n",
    "from sklearn.metrics import mean_squared_error        # Import mean_squared_error function from scikit-learn for error calculation\n",
    "\n",
    "\n",
    "# Load the dataset 'concrete_data.csv' into a Pandas DataFrame\n",
    "concrete_data = pd.read_csv('concrete_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "concrete_data.head()\n",
    "\n",
    "# Display the shape of the dataset (rows, columns)\n",
    "concrete_data.shape\n",
    "\n",
    "# Display summary statistics of the dataset\n",
    "concrete_data.describe()\n",
    "\n",
    "# Check for missing values in the dataset and sum them up\n",
    "concrete_data.isnull().sum()\n",
    "\n",
    "# Extract column names from the dataset\n",
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "# Separate predictors (all columns except 'Strength') and target ('Strength' column)\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']]\n",
    "target = concrete_data['Strength']\n",
    "\n",
    "# Display the first few rows of predictors and target\n",
    "predictors.head()\n",
    "target.head()\n",
    "\n",
    "# Determine the number of predictors (number of columns excluding the target variable)\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Display the number of predictors\n",
    "n_cols\n",
    "\n",
    "# Define a function to create a regression model using Keras Sequential API\n",
    "def regression_model():\n",
    "    model = Sequential()                       # Create a Sequential model\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))  # Add a Dense layer with 10 neurons, ReLU activation, input shape based on number of predictors\n",
    "    model.add(Dense(1))                        # Add an output Dense layer with 1 neuron (for regression tasks)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Compile the model with Adam optimizer and mean squared error loss\n",
    "    return model\n",
    "\n",
    "# Split data into training and testing sets using train_test_split from scikit-learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a regression model\n",
    "model = regression_model()\n",
    "\n",
    "# Set number of epochs for training\n",
    "epochs = 50\n",
    "\n",
    "# Fit the model on training data\n",
    "model.fit(X_train, y_train, epochs=epochs, verbose=1)\n",
    "\n",
    "# Evaluate the model on testing data and get the loss value\n",
    "loss_val = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predict values using the model on testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the loss value\n",
    "loss_val\n",
    "\n",
    "# Calculate mean squared error between predicted and actual values\n",
    "mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate mean and standard deviation of mean squared errors\n",
    "mean = np.mean(mean_square_error)\n",
    "standard_deviation = np.std(mean_square_error)\n",
    "\n",
    "# Print mean and standard deviation of mean squared errors\n",
    "print(mean, standard_deviation)\n",
    "\n",
    "# Perform repeated training and evaluation with different random states to compute mean squared errors\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mean_squared_errors = []\n",
    "\n",
    "# Loop through each random state, split data, train model, and calculate MSE\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "    MSE = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error)\n",
    "\n",
    "# Convert mean squared errors list into NumPy array\n",
    "mean_squared_errors = np.array(mean_squared_errors)\n",
    "\n",
    "# Calculate mean and standard deviation of mean squared errors\n",
    "mean = np.mean(mean_squared_errors)\n",
    "standard_deviation = np.std(mean_squared_errors)\n",
    "\n",
    "# Print results including mean and standard deviation of mean squared errors\n",
    "print('\\n')\n",
    "print(\"Below is the mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors without normalized data. Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean))\n",
    "print(\"Standard Deviation: \"+str(standard_deviation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa818d9-ffbb-4e53-a94e-d904bbe17c5e",
   "metadata": {},
   "source": [
    "**Part B:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af88f734-93a4-426a-a00b-4c9fde9d8faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 8s - loss: 1572.8956\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1555.3920\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1537.7578\n",
      "Epoch 4/50\n",
      " - 0s - loss: 1519.5663\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1500.9362\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1481.8006\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1461.8723\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1441.2800\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1419.8196\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1397.4047\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1374.1188\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1350.1107\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1325.1567\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1299.6282\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1272.8250\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1245.7000\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1217.4924\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1189.1355\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1159.6208\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1129.9229\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1099.8737\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1069.2896\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1038.1784\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1007.5656\n",
      "Epoch 25/50\n",
      " - 0s - loss: 976.0241\n",
      "Epoch 26/50\n",
      " - 0s - loss: 944.4154\n",
      "Epoch 27/50\n",
      " - 0s - loss: 913.0799\n",
      "Epoch 28/50\n",
      " - 0s - loss: 881.8493\n",
      "Epoch 29/50\n",
      " - 0s - loss: 850.3452\n",
      "Epoch 30/50\n",
      " - 0s - loss: 819.2372\n",
      "Epoch 31/50\n",
      " - 0s - loss: 788.7127\n",
      "Epoch 32/50\n",
      " - 0s - loss: 758.3916\n",
      "Epoch 33/50\n",
      " - 0s - loss: 728.3196\n",
      "Epoch 34/50\n",
      " - 0s - loss: 698.7811\n",
      "Epoch 35/50\n",
      " - 0s - loss: 670.3470\n",
      "Epoch 36/50\n",
      " - 0s - loss: 642.1425\n",
      "Epoch 37/50\n",
      " - 0s - loss: 614.8780\n",
      "Epoch 38/50\n",
      " - 0s - loss: 588.4358\n",
      "Epoch 39/50\n",
      " - 0s - loss: 562.8465\n",
      "Epoch 40/50\n",
      " - 0s - loss: 538.2629\n",
      "Epoch 41/50\n",
      " - 0s - loss: 513.6376\n",
      "Epoch 42/50\n",
      " - 0s - loss: 490.6511\n",
      "Epoch 43/50\n",
      " - 0s - loss: 468.7322\n",
      "Epoch 44/50\n",
      " - 0s - loss: 447.3307\n",
      "Epoch 45/50\n",
      " - 0s - loss: 427.6941\n",
      "Epoch 46/50\n",
      " - 0s - loss: 408.1092\n",
      "Epoch 47/50\n",
      " - 0s - loss: 390.3006\n",
      "Epoch 48/50\n",
      " - 0s - loss: 373.3040\n",
      "Epoch 49/50\n",
      " - 0s - loss: 356.6785\n",
      "Epoch 50/50\n",
      " - 0s - loss: 341.7703\n",
      "309/309 [==============================] - 4s 12ms/step\n",
      "334.2243360386655 0.0\n",
      "MSE 1: 156.56859181228194\n",
      "MSE 2: 147.77783948781035\n",
      "MSE 3: 92.84279965891422\n",
      "MSE 4: 67.81366659368126\n",
      "MSE 5: 53.89026559132203\n",
      "MSE 6: 53.81313602978358\n",
      "MSE 7: 49.11709288563157\n",
      "MSE 8: 37.01463566011596\n",
      "MSE 9: 38.23732442763245\n",
      "MSE 10: 38.66005191988158\n",
      "MSE 11: 36.913535096498755\n",
      "MSE 12: 33.18688530906505\n",
      "MSE 13: 40.92774461542518\n",
      "MSE 14: 39.96774522849271\n",
      "MSE 15: 33.51986590635429\n",
      "MSE 16: 30.109203357140995\n",
      "MSE 17: 33.557177336856384\n",
      "MSE 18: 34.08730679113888\n",
      "MSE 19: 31.757399074468026\n",
      "MSE 20: 31.559923486802184\n",
      "MSE 21: 29.308836075866108\n",
      "MSE 22: 30.219753296244107\n",
      "MSE 23: 28.170824217564853\n",
      "MSE 24: 29.439226082613555\n",
      "MSE 25: 32.82939710894835\n",
      "MSE 26: 33.560859482651004\n",
      "MSE 27: 27.931275105399223\n",
      "MSE 28: 29.308504444109968\n",
      "MSE 29: 33.904133657807286\n",
      "MSE 30: 31.003972433145766\n",
      "MSE 31: 28.129516848468473\n",
      "MSE 32: 26.602302761139608\n",
      "MSE 33: 26.65225304909123\n",
      "MSE 34: 30.47503432486821\n",
      "MSE 35: 29.68185777880227\n",
      "MSE 36: 33.456080131160405\n",
      "MSE 37: 27.141071348900162\n",
      "MSE 38: 31.490032961453434\n",
      "MSE 39: 29.873943464655706\n",
      "MSE 40: 25.047798848075004\n",
      "MSE 41: 33.874906533744344\n",
      "MSE 42: 26.636432555115338\n",
      "MSE 43: 28.443908302529344\n",
      "MSE 44: 34.411905245487745\n",
      "MSE 45: 30.702186090660714\n",
      "MSE 46: 30.13275787205372\n",
      "MSE 47: 28.753924051920574\n",
      "MSE 48: 30.15853718877996\n",
      "MSE 49: 31.19422411378533\n",
      "MSE 50: 30.633343236731864\n",
      "\n",
      "\n",
      "Below is the mean and standard deviation of 50 mean squared errors with normalized data. Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 39.60981976737905\n",
      "Standard Deviation: 25.629549394479426\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd               # Import Pandas for data manipulation\n",
    "import numpy as np                # Import NumPy for numerical operations\n",
    "import keras                     # Import Keras for building neural networks\n",
    "from keras.models import Sequential  # Import Sequential model from Keras\n",
    "from keras.layers import Dense       # Import Dense layer from Keras\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split from scikit-learn\n",
    "from sklearn.metrics import mean_squared_error        # Import mean_squared_error from scikit-learn\n",
    "\n",
    "\n",
    "# Load the concrete strength dataset from 'concrete_data.csv' into a Pandas DataFrame\n",
    "concrete_data = pd.read_csv('concrete_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "concrete_data.head()\n",
    "\n",
    "# Display the shape of the dataset (rows, columns)\n",
    "concrete_data.shape\n",
    "\n",
    "# Display summary statistics of the dataset\n",
    "concrete_data.describe()\n",
    "\n",
    "# Check for missing values in the dataset and sum them up\n",
    "concrete_data.isnull().sum()\n",
    "\n",
    "# Extract column names from the dataset\n",
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "# Separate predictors (all columns except 'Strength') and target ('Strength' column)\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']]\n",
    "target = concrete_data['Strength']\n",
    "\n",
    "# Display the first few rows of predictors and target\n",
    "predictors.head()\n",
    "target.head()\n",
    "\n",
    "# Normalize predictors using z-score normalization\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors_norm.head()\n",
    "\n",
    "# Determine the number of predictors after normalization\n",
    "n_cols = predictors_norm.shape[1]\n",
    "\n",
    "# Display the number of predictors\n",
    "n_cols\n",
    "\n",
    "# Define a function to create a regression model using Keras Sequential API\n",
    "def regression_model():\n",
    "    model = Sequential()                       # Create a Sequential model\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))  # Add a Dense layer with 10 neurons, ReLU activation, input shape based on number of predictors\n",
    "    model.add(Dense(1))                        # Add an output Dense layer with 1 neuron (for regression tasks)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Compile the model with Adam optimizer and mean squared error loss\n",
    "    return model\n",
    "\n",
    "# Split normalized data into training and testing sets using train_test_split from scikit-learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a regression model\n",
    "model = regression_model()\n",
    "\n",
    "# Set number of epochs for training\n",
    "epochs = 50\n",
    "\n",
    "# Fit the model on training data and display training progress\n",
    "model.fit(X_train, y_train, epochs=epochs, verbose=2)\n",
    "\n",
    "# Evaluate the model on testing data and get the loss value\n",
    "loss_val = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predict values using the model on testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the loss value\n",
    "loss_val\n",
    "\n",
    "# Calculate mean squared error between predicted and actual values\n",
    "mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate mean and standard deviation of mean squared errors\n",
    "mean = np.mean(mean_square_error)\n",
    "standard_deviation = np.std(mean_square_error)\n",
    "\n",
    "# Print mean and standard deviation of mean squared errors\n",
    "print(mean, standard_deviation)\n",
    "\n",
    "# Perform repeated training and evaluation with different random states to compute mean squared errors\n",
    "total_mean_squared_errors = 50\n",
    "epochs = 50\n",
    "mean_squared_errors = []\n",
    "\n",
    "# Loop through each random state, split data, train model, and calculate MSE\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "    MSE = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error)\n",
    "\n",
    "# Convert mean squared errors list into NumPy array\n",
    "mean_squared_errors = np.array(mean_squared_errors)\n",
    "\n",
    "# Calculate mean and standard deviation of mean squared errors\n",
    "mean = np.mean(mean_squared_errors)\n",
    "standard_deviation = np.std(mean_squared_errors)\n",
    "\n",
    "# Print results including mean and standard deviation of mean squared errors\n",
    "print('\\n')\n",
    "print(\"Below is the mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean))\n",
    "print(\"Standard Deviation: \"+str(standard_deviation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b6ecf-1e0e-4bdc-81da-6d57aa6e8637",
   "metadata": {},
   "source": [
    "**Part C:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4701d346-941d-42e5-bf4c-f7fc9abe913a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 8s - loss: 1578.0860\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1562.7455\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1547.0775\n",
      "Epoch 4/50\n",
      " - 0s - loss: 1531.0281\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1514.4482\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1496.8344\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1478.3424\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1458.6491\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1438.0490\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1416.2415\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1393.2521\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1369.3212\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1344.2085\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1318.1747\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1291.2597\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1263.2376\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1234.3684\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1204.5547\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1173.9645\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1142.3352\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1109.5540\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1076.5359\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1042.6374\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1008.4422\n",
      "Epoch 25/50\n",
      " - 0s - loss: 973.1050\n",
      "Epoch 26/50\n",
      " - 0s - loss: 937.9165\n",
      "Epoch 27/50\n",
      " - 0s - loss: 903.5074\n",
      "Epoch 28/50\n",
      " - 0s - loss: 868.5985\n",
      "Epoch 29/50\n",
      " - 0s - loss: 834.4139\n",
      "Epoch 30/50\n",
      " - 0s - loss: 800.4659\n",
      "Epoch 31/50\n",
      " - 0s - loss: 766.9224\n",
      "Epoch 32/50\n",
      " - 0s - loss: 734.3655\n",
      "Epoch 33/50\n",
      " - 0s - loss: 702.2826\n",
      "Epoch 34/50\n",
      " - 0s - loss: 670.8974\n",
      "Epoch 35/50\n",
      " - 0s - loss: 640.5537\n",
      "Epoch 36/50\n",
      " - 0s - loss: 611.1487\n",
      "Epoch 37/50\n",
      " - 0s - loss: 583.0555\n",
      "Epoch 38/50\n",
      " - 0s - loss: 555.6128\n",
      "Epoch 39/50\n",
      " - 0s - loss: 530.3411\n",
      "Epoch 40/50\n",
      " - 0s - loss: 505.3542\n",
      "Epoch 41/50\n",
      " - 0s - loss: 481.8643\n",
      "Epoch 42/50\n",
      " - 0s - loss: 460.0559\n",
      "Epoch 43/50\n",
      " - 0s - loss: 439.3461\n",
      "Epoch 44/50\n",
      " - 0s - loss: 420.1013\n",
      "Epoch 45/50\n",
      " - 0s - loss: 402.3690\n",
      "Epoch 46/50\n",
      " - 0s - loss: 385.4224\n",
      "Epoch 47/50\n",
      " - 0s - loss: 370.2370\n",
      "Epoch 48/50\n",
      " - 0s - loss: 355.7069\n",
      "Epoch 49/50\n",
      " - 0s - loss: 342.5071\n",
      "Epoch 50/50\n",
      " - 0s - loss: 330.3803\n",
      "309/309 [==============================] - 4s 12ms/step\n",
      "305.4266286226091 0.0\n",
      "MSE 1: 105.16351952907723\n",
      "MSE 2: 89.10588456434725\n",
      "MSE 3: 65.74274579143832\n",
      "MSE 4: 49.14010055937042\n",
      "MSE 5: 43.714822207453956\n",
      "MSE 6: 46.33078163418569\n",
      "MSE 7: 45.848691378596534\n",
      "MSE 8: 34.402883079059684\n",
      "MSE 9: 38.32099679990108\n",
      "MSE 10: 38.444296136257336\n",
      "MSE 11: 38.9781775119621\n",
      "MSE 12: 34.39568030101196\n",
      "MSE 13: 42.4554926184003\n",
      "MSE 14: 43.074445076359126\n",
      "MSE 15: 35.467425793891586\n",
      "MSE 16: 32.227178555090454\n",
      "MSE 17: 36.93153847999943\n",
      "MSE 18: 36.68783665012002\n",
      "MSE 19: 35.66809890648308\n",
      "MSE 20: 36.263797056327746\n",
      "MSE 21: 34.06528717337303\n",
      "MSE 22: 35.58606571209855\n",
      "MSE 23: 31.093999263923916\n",
      "MSE 24: 34.35660276443827\n",
      "MSE 25: 36.98907533664148\n",
      "MSE 26: 37.07752756161983\n",
      "MSE 27: 33.594633256733225\n",
      "MSE 28: 32.93572943110296\n",
      "MSE 29: 39.849447910839686\n",
      "MSE 30: 37.74684364047251\n",
      "MSE 31: 32.575869304076754\n",
      "MSE 32: 33.27259492102564\n",
      "MSE 33: 32.312184318369646\n",
      "MSE 34: 36.03081339616992\n",
      "MSE 35: 34.93599251558866\n",
      "MSE 36: 41.55385725397894\n",
      "MSE 37: 31.588379504996983\n",
      "MSE 38: 37.630167988897526\n",
      "MSE 39: 35.41262206290532\n",
      "MSE 40: 31.28674525041796\n",
      "MSE 41: 37.37675637797631\n",
      "MSE 42: 29.626773044129404\n",
      "MSE 43: 35.88372857670954\n",
      "MSE 44: 37.88491957087347\n",
      "MSE 45: 37.31835156042599\n",
      "MSE 46: 36.878501867399244\n",
      "MSE 47: 33.55870409227884\n",
      "MSE 48: 36.949090432966415\n",
      "MSE 49: 36.50876254479862\n",
      "MSE 50: 38.94546593971623\n",
      "\n",
      "\n",
      "Below is the mean and standard deviation of 50 mean squared errors with normalized data. Total number of epochs for each training is: 100\n",
      "\n",
      "Mean: 39.783797309430845\n",
      "Standard Deviation: 13.100607183062845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd               # Import Pandas for data manipulation\n",
    "import numpy as np                # Import NumPy for numerical operations\n",
    "import keras                     # Import Keras for building neural networks\n",
    "from keras.models import Sequential  # Import Sequential model from Keras\n",
    "from keras.layers import Dense       # Import Dense layer from Keras\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split from scikit-learn\n",
    "from sklearn.metrics import mean_squared_error        # Import mean_squared_error from scikit-learn\n",
    "\n",
    "\n",
    "concrete_data = pd.read_csv('concrete_data.csv')  # Load concrete data from CSV file into a Pandas DataFrame\n",
    "concrete_data.head()                              # Display the first few rows of the dataset\n",
    "\n",
    "concrete_data.shape                              # Display the shape of the dataset (rows, columns)\n",
    "concrete_data.describe()                         # Display summary statistics of the dataset\n",
    "concrete_data.isnull().sum()                     # Check for missing values in the dataset and sum them up\n",
    "\n",
    "concrete_data_columns = concrete_data.columns    # Extract column names from the dataset\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']]  # Separate predictors (all columns except 'Strength')\n",
    "target = concrete_data['Strength']               # Define the target variable as 'Strength'\n",
    "\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()  # Normalize predictors using z-score normalization\n",
    "predictors_norm.head()                           # Display the first few rows of normalized predictors\n",
    "n_cols = predictors_norm.shape[1]                # Determine the number of predictors after normalization\n",
    "\n",
    "def regression_model():\n",
    "    model = Sequential()                         # Create a Sequential model\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))  # Add a Dense layer with 10 neurons, ReLU activation, input shape based on number of predictors\n",
    "    model.add(Dense(1))                          # Add an output Dense layer with 1 neuron (for regression tasks)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Compile the model with Adam optimizer and mean squared error loss\n",
    "    return model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=42)  # Split normalized data into training and testing sets\n",
    "\n",
    "model = regression_model()                       # Create a regression model using the defined function\n",
    "\n",
    "epochs = 100                                     # Set number of epochs for training\n",
    "model.fit(X_train, y_train, epochs=epochs, verbose=2)  # Fit the model on training data with verbose output\n",
    "\n",
    "loss_val = model.evaluate(X_test, y_test)        # Evaluate the model on testing data and get the loss value\n",
    "y_pred = model.predict(X_test)                   # Predict values using the model on testing data\n",
    "\n",
    "loss_val                                         # Display the loss value\n",
    "\n",
    "mean_square_error = mean_squared_error(y_test, y_pred)  # Calculate mean squared error between predicted and actual values\n",
    "mean = np.mean(mean_square_error)                 # Calculate mean of mean squared errors\n",
    "standard_deviation = np.std(mean_square_error)    # Calculate standard deviation of mean squared errors\n",
    "\n",
    "print(mean, standard_deviation)                   # Print mean and standard deviation of mean squared errors\n",
    "\n",
    "total_mean_squared_errors = 50                    # Set total number of mean squared errors to compute\n",
    "epochs = 100                                     \n",
    "mean_squared_errors = []\n",
    "\n",
    "# Loop through each random state, split data, train model, and calculate MSE for multiple runs\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "    MSE = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))        # Print MSE for each iteration\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error) # Append MSE to list\n",
    "\n",
    "mean_squared_errors = np.array(mean_squared_errors)  # Convert list of MSEs to NumPy array\n",
    "mean = np.mean(mean_squared_errors)                # Calculate mean of all MSEs\n",
    "standard_deviation = np.std(mean_squared_errors)   # Calculate standard deviation of all MSEs\n",
    "\n",
    "print('\\n')\n",
    "print(\"Below is the mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean))                         # Print mean of all MSEs\n",
    "print(\"Standard Deviation: \"+str(standard_deviation))  # Print standard deviation of all MSEs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ab188-6bec-4b6b-9c03-bdc060271a5f",
   "metadata": {},
   "source": [
    "**Part D:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf9ac86-ab09-4b6d-91ed-7aeb216e48d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 8s - loss: 1600.5999\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1581.2430\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1563.8625\n",
      "Epoch 4/50\n",
      " - 0s - loss: 1541.8916\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1510.4154\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1463.3642\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1391.9230\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1282.4421\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1101.2515\n",
      "Epoch 10/50\n",
      " - 0s - loss: 840.1647\n",
      "Epoch 11/50\n",
      " - 0s - loss: 543.8152\n",
      "Epoch 12/50\n",
      " - 0s - loss: 330.0385\n",
      "Epoch 13/50\n",
      " - 0s - loss: 244.7133\n",
      "Epoch 14/50\n",
      " - 0s - loss: 221.4331\n",
      "Epoch 15/50\n",
      " - 0s - loss: 210.4879\n",
      "Epoch 16/50\n",
      " - 0s - loss: 202.8366\n",
      "Epoch 17/50\n",
      " - 0s - loss: 195.8784\n",
      "Epoch 18/50\n",
      " - 0s - loss: 190.3919\n",
      "Epoch 19/50\n",
      " - 0s - loss: 185.9796\n",
      "Epoch 20/50\n",
      " - 0s - loss: 181.7075\n",
      "Epoch 21/50\n",
      " - 0s - loss: 177.9545\n",
      "Epoch 22/50\n",
      " - 0s - loss: 174.9440\n",
      "Epoch 23/50\n",
      " - 0s - loss: 171.5687\n",
      "Epoch 24/50\n",
      " - 0s - loss: 168.3784\n",
      "Epoch 25/50\n",
      " - 0s - loss: 166.0783\n",
      "Epoch 26/50\n",
      " - 0s - loss: 163.4098\n",
      "Epoch 27/50\n",
      " - 0s - loss: 161.1111\n",
      "Epoch 28/50\n",
      " - 0s - loss: 158.5945\n",
      "Epoch 29/50\n",
      " - 0s - loss: 156.7447\n",
      "Epoch 30/50\n",
      " - 0s - loss: 154.7056\n",
      "Epoch 31/50\n",
      " - 0s - loss: 153.0439\n",
      "Epoch 32/50\n",
      " - 0s - loss: 151.4753\n",
      "Epoch 33/50\n",
      " - 0s - loss: 149.7627\n",
      "Epoch 34/50\n",
      " - 0s - loss: 148.4178\n",
      "Epoch 35/50\n",
      " - 0s - loss: 147.1019\n",
      "Epoch 36/50\n",
      " - 0s - loss: 145.6641\n",
      "Epoch 37/50\n",
      " - 0s - loss: 144.4179\n",
      "Epoch 38/50\n",
      " - 0s - loss: 143.4344\n",
      "Epoch 39/50\n",
      " - 0s - loss: 142.3324\n",
      "Epoch 40/50\n",
      " - 0s - loss: 141.5504\n",
      "Epoch 41/50\n",
      " - 0s - loss: 139.9324\n",
      "Epoch 42/50\n",
      " - 0s - loss: 138.9946\n",
      "Epoch 43/50\n",
      " - 0s - loss: 138.2919\n",
      "Epoch 44/50\n",
      " - 0s - loss: 137.4841\n",
      "Epoch 45/50\n",
      " - 0s - loss: 136.2629\n",
      "Epoch 46/50\n",
      " - 0s - loss: 135.4736\n",
      "Epoch 47/50\n",
      " - 0s - loss: 134.8312\n",
      "Epoch 48/50\n",
      " - 0s - loss: 133.8981\n",
      "Epoch 49/50\n",
      " - 0s - loss: 133.3213\n",
      "Epoch 50/50\n",
      " - 0s - loss: 132.6014\n",
      "309/309 [==============================] - 4s 12ms/step\n",
      "128.51574811580645 0.0\n",
      "MSE 1: 103.57392421586614\n",
      "MSE 2: 119.52732538019569\n",
      "MSE 3: 80.59078539999558\n",
      "MSE 4: 57.91181684080451\n",
      "MSE 5: 44.4639585674002\n",
      "MSE 6: 46.195403003383994\n",
      "MSE 7: 42.97978575870057\n",
      "MSE 8: 32.47428936016984\n",
      "MSE 9: 35.14518973588172\n",
      "MSE 10: 33.636498250621806\n",
      "MSE 11: 34.09876754291621\n",
      "MSE 12: 27.100207628169876\n",
      "MSE 13: 35.55708670384676\n",
      "MSE 14: 33.3993675330696\n",
      "MSE 15: 32.13486997980902\n",
      "MSE 16: 22.242988882712947\n",
      "MSE 17: 28.763409685548456\n",
      "MSE 18: 29.328835289840946\n",
      "MSE 19: 28.373927443544456\n",
      "MSE 20: 33.013205235058436\n",
      "MSE 21: 26.512121743754662\n",
      "MSE 22: 30.73861075836478\n",
      "MSE 23: 26.037676814304586\n",
      "MSE 24: 31.43086466125686\n",
      "MSE 25: 33.64505167223489\n",
      "MSE 26: 31.728661645191774\n",
      "MSE 27: 27.47740117786\n",
      "MSE 28: 28.06082402695344\n",
      "MSE 29: 32.09957091939488\n",
      "MSE 30: 28.25927551047316\n",
      "MSE 31: 23.532807439662108\n",
      "MSE 32: 25.560060544307177\n",
      "MSE 33: 22.367749748106526\n",
      "MSE 34: 26.725738605635065\n",
      "MSE 35: 27.515343848169813\n",
      "MSE 36: 33.20201589220164\n",
      "MSE 37: 24.35621913119813\n",
      "MSE 38: 26.03770683807077\n",
      "MSE 39: 25.889635456418528\n",
      "MSE 40: 22.77386398068524\n",
      "MSE 41: 26.593969950012404\n",
      "MSE 42: 23.211151215636615\n",
      "MSE 43: 26.66521506324941\n",
      "MSE 44: 27.279317127462345\n",
      "MSE 45: 29.100122976457417\n",
      "MSE 46: 26.453693476309667\n",
      "MSE 47: 25.961630997148532\n",
      "MSE 48: 28.367738939797608\n",
      "MSE 49: 24.56289291381836\n",
      "MSE 50: 26.290508943082447\n",
      "\n",
      "\n",
      "Below is the mean and standard deviation of 50 mean squared errors with normalized data. Total number of epochs for each training is: 50\n",
      "\n",
      "Mean: 34.378981126379756\n",
      "Standard Deviation: 18.50386181770321\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd               # Import Pandas for data manipulation\n",
    "import numpy as np                # Import NumPy for numerical operations\n",
    "import keras                     # Import Keras for building neural networks\n",
    "from keras.models import Sequential  # Import Sequential model from Keras\n",
    "from keras.layers import Dense       # Import Dense layer from Keras\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split from scikit-learn\n",
    "from sklearn.metrics import mean_squared_error        # Import mean_squared_error from scikit-learn\n",
    "\n",
    "\n",
    "concrete_data = pd.read_csv('concrete_data.csv')  # Load concrete data from CSV file into a Pandas DataFrame\n",
    "concrete_data.head()                              # Display the first few rows of the dataset\n",
    "\n",
    "concrete_data.shape                              # Display the shape of the dataset (rows, columns)\n",
    "concrete_data.describe()                         # Display summary statistics of the dataset\n",
    "concrete_data.isnull().sum()                     # Check for missing values in the dataset and sum them up\n",
    "\n",
    "concrete_data_columns = concrete_data.columns    # Extract column names from the dataset\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']]  # Separate predictors (all columns except 'Strength')\n",
    "target = concrete_data['Strength']               # Define the target variable as 'Strength'\n",
    "\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()  # Normalize predictors using z-score normalization\n",
    "predictors_norm.head()                           # Display the first few rows of normalized predictors\n",
    "n_cols = predictors_norm.shape[1]                # Determine the number of predictors after normalization\n",
    "\n",
    "# Define a function to create a regression model\n",
    "def regression_model():\n",
    "    model = Sequential()                         # Create a Sequential model\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))  # Add a Dense layer with 10 neurons, ReLU activation, input shape based on number of predictors\n",
    "    model.add(Dense(10, activation='relu'))      # Add another Dense layer with 10 neurons and ReLU activation\n",
    "    model.add(Dense(10, activation='relu'))      # Add another Dense layer with 10 neurons and ReLU activation\n",
    "    model.add(Dense(1))                          # Add an output Dense layer with 1 neuron (for regression tasks)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')  # Compile the model with Adam optimizer and mean squared error loss\n",
    "    return model\n",
    "\n",
    "# Split normalized data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=42)\n",
    "\n",
    "model = regression_model()                       # Create a regression model using the defined function\n",
    "\n",
    "epochs = 50                                      # Set number of epochs for training\n",
    "model.fit(X_train, y_train, epochs=epochs, verbose=2)  # Fit the model on training data with verbose output\n",
    "\n",
    "loss_val = model.evaluate(X_test, y_test)        # Evaluate the model on testing data and get the loss value\n",
    "y_pred = model.predict(X_test)                   # Predict values using the model on testing data\n",
    "\n",
    "loss_val                                         # Display the loss value\n",
    "\n",
    "mean_square_error = mean_squared_error(y_test, y_pred)  # Calculate mean squared error between predicted and actual values\n",
    "mean = np.mean(mean_square_error)                 # Calculate mean of mean squared errors\n",
    "standard_deviation = np.std(mean_square_error)    # Calculate standard deviation of mean squared errors\n",
    "\n",
    "print(mean, standard_deviation)                   # Print mean and standard deviation of mean squared errors\n",
    "\n",
    "total_mean_squared_errors = 50                    # Set total number of mean squared errors to compute\n",
    "epochs = 50                                     \n",
    "mean_squared_errors = []\n",
    "\n",
    "# Loop through each random state, split data, train model, and calculate MSE for multiple runs\n",
    "for i in range(0, total_mean_squared_errors):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=0)\n",
    "    MSE = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"MSE \"+str(i+1)+\": \"+str(MSE))        # Print MSE for each iteration\n",
    "    y_pred = model.predict(X_test)\n",
    "    mean_square_error = mean_squared_error(y_test, y_pred)\n",
    "    mean_squared_errors.append(mean_square_error) # Append MSE to list\n",
    "\n",
    "mean_squared_errors = np.array(mean_squared_errors)  # Convert list of MSEs to NumPy array\n",
    "mean = np.mean(mean_squared_errors)                # Calculate mean of all MSEs\n",
    "standard_deviation = np.std(mean_squared_errors)   # Calculate standard deviation of all MSEs\n",
    "\n",
    "print('\\n')\n",
    "print(\"Below is the mean and standard deviation of \" +str(total_mean_squared_errors) + \" mean squared errors with normalized data. Total number of epochs for each training is: \" +str(epochs) + \"\\n\")\n",
    "print(\"Mean: \"+str(mean))                         # Print mean of all MSEs\n",
    "print(\"Standard Deviation: \"+str(standard_deviation))  # Print standard deviation of all MSEs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
